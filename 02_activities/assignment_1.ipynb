{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n",
      "cannot find .env file\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "256159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.hbr.org\n",
      "B\n",
      " \n",
      "EST  \n",
      " \n",
      "OF  HBR 1999\n",
      " \n",
      "Managing Oneself\n",
      " \n",
      "by Peter F . Drucker\n",
      " \n",
      "â€¢\n",
      " \n",
      "Included with this full-text \n",
      " \n",
      "Harvard Business Review\n",
      " \n",
      " article:\n",
      "The Idea in Briefâ€”the core idea\n",
      "The Idea in Practiceâ€”putting the idea to work\n",
      " \n",
      "1\n",
      " \n",
      "Article Summary\n",
      " \n",
      "2\n",
      " \n",
      "Managing Oneself\n",
      "A list of related materials, with annotations to guide further\n",
      "exploration of the articleâ€™s ideas and applications\n",
      " \n",
      "12\n",
      " \n",
      "Further Reading\n",
      "Success in the knowledge \n",
      "economy comes to those who \n",
      "know themselvesâ€”their \n",
      "strengths, their values, and \n",
      "how they best perform.\n",
      " \n",
      "Reprint R0501KThis document is authorized for use only by Sharon Brooks (SHARON@PRICE-ASSOCIATES.COM). Copying or posting is an infringement of copyright. Please contact \n",
      "customerservice@harvardbusiness.org or 800-988-0886 for additional copies.\n",
      "B\n",
      " \n",
      "EST\n",
      " \n",
      " \n",
      " \n",
      "OF\n",
      " \n",
      " HBR 1999\n",
      " \n",
      "Managing Oneself\n",
      " \n",
      "page 1\n",
      " \n",
      "The Idea in Brief The Idea in Practice\n",
      " \n",
      "COPYRIGHT Â© 2004 HARVARD BUSINESS SCHOOL PUBLISHING CORPORATION. ALL RIGHTS RESERVED.\n",
      " \n",
      "We live in an age of\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the PyPDFLoader from LangChain community loaders\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# âœ… Step 1: Define path to your PDF file\n",
    "# Use a raw string to prevent backslash escape errors on Windows\n",
    "file_path = r\"Managing Oneself_Drucker_HBR.pdf\"\n",
    "\n",
    "# âœ… Step 2: Load the PDF document\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# âœ… Step 3: Combine all pages into one continuous text string\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "\n",
    "# âœ… Step 4: Preview the first 1000 characters of extracted text\n",
    "print(document_text[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     OutputTokens: \u001b[38;5;28mint\u001b[39m = Field(description=\u001b[33m\"\u001b[39m\u001b[33mNumber of output tokens generated.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# âœ… Load API key and client\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m client = \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Define system and user prompts\u001b[39;00m\n\u001b[32m     18\u001b[39m system_prompt = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[33mYou are an expert summarizer. Analyze the provided document text and generate a structured summary\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[33musing the specified tone and the required schema. Do not invent data outside the text.\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vidhi\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\openai\\_client.py:137\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    135\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    138\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m     )\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[32m    141\u001b[39m     \u001b[38;5;28mself\u001b[39m.api_key = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "\n",
    "# Define structured schema\n",
    "class ArticleSummary(BaseModel):\n",
    "    Author: str = Field(description=\"The author of the article or book.\")\n",
    "    Title: str = Field(description=\"The title of the work being summarized.\")\n",
    "    Relevance: str = Field(description=\"Why this article is relevant for an AI professional.\")\n",
    "    Summary: str = Field(description=\"A concise summary no longer than 1000 tokens.\")\n",
    "    Tone: str = Field(description=\"The tone or style used for the summary output, e.g., 'Legalese' or 'Victorian English'.\")\n",
    "    InputTokens: int = Field(description=\"Number of input tokens.\")\n",
    "    OutputTokens: int = Field(description=\"Number of output tokens generated.\")\n",
    "\n",
    "# âœ… Load API key and client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define system and user prompts\n",
    "system_prompt = \"\"\"\n",
    "You are an expert summarizer. Analyze the provided document text and generate a structured summary\n",
    "using the specified tone and the required schema. Do not invent data outside the text.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Here is the document content:\n",
    "\n",
    "{document_text[:6000]}\n",
    "\n",
    "Write the summary in a clearly distinguishable tone such as Victorian English.\n",
    "\"\"\"\n",
    "\n",
    "# âœ… Generate structured output\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o\",   # Not in GPT-5 family\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    "    response_format=ArticleSummary\n",
    ")\n",
    "\n",
    "# âœ… Extract parsed Pydantic object\n",
    "structured_output = response.output_parsed\n",
    "print(structured_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99560b73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'structured_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Step 3: Define your test case (example input and summary)\u001b[39;00m\n\u001b[32m     19\u001b[39m input_text = document_text[:\u001b[32m6000\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m actual_output = \u001b[43mstructured_output\u001b[49m.Summary\n\u001b[32m     22\u001b[39m test_case = LLMTestCase(\n\u001b[32m     23\u001b[39m     \u001b[38;5;28minput\u001b[39m=input_text,\n\u001b[32m     24\u001b[39m     actual_output=actual_output\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Step 4: Create the Summarization Metric with bespoke assessment questions\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'structured_output' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Imports\n",
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Step 2: Define the output schema\n",
    "class EvaluationResults(BaseModel):\n",
    "    SummarizationScore: float = Field(..., description=\"Summarization metric score (0 to 1).\")\n",
    "    SummarizationReason: str = Field(..., description=\"Explanation provided by the summarization metric.\")\n",
    "    CoherenceScore: float = Field(..., description=\"Coherence or clarity score (0 to 1).\")\n",
    "    CoherenceReason: str = Field(..., description=\"Explanation of the Coherence score.\")\n",
    "    TonalityScore: float = Field(..., description=\"Tonality metric score (0 to 1).\")\n",
    "    TonalityReason: str = Field(..., description=\"Explanation of Tonality score.\")\n",
    "    SafetyScore: float = Field(..., description=\"Safety metric score (0 to 1).\")\n",
    "    SafetyReason: str = Field(..., description=\"Explanation of Safety score.\")\n",
    "\n",
    "# Step 3: Define your test case (example input and summary)\n",
    "input_text = document_text[:6000]\n",
    "actual_output = structured_output.Summary\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=input_text,\n",
    "    actual_output=actual_output\n",
    ")\n",
    "\n",
    "# Step 4: Create the Summarization Metric with bespoke assessment questions\n",
    "summarization_metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",  # not GPT-5 family\n",
    "    assessment_questions=[\n",
    "        \"Does the summary accurately reflect the facts in the original text?\",\n",
    "        \"Does the summary avoid omitting critical details?\",\n",
    "        \"Does the summary maintain factual consistency?\",\n",
    "        \"Is the summary concise yet complete?\",\n",
    "        \"Does the summary include all essential information?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Step 5: Define the three G-Eval metrics\n",
    "\n",
    "# --- Coherence Metric ---\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    model=\"gpt-4o\",\n",
    "    evaluation_steps=[\n",
    "        \"Determine whether the response uses clear and direct language.\",\n",
    "        \"Assess if the explanation avoids jargon or clarifies it when used.\",\n",
    "        \"Evaluate whether ideas are logically connected and easy to follow.\",\n",
    "        \"Check if any parts are confusing or disjointed.\",\n",
    "        \"Ensure the text maintains grammatical consistency.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "# --- Tonality Metric ---\n",
    "tonality_metric = GEval(\n",
    "    name=\"Tonality\",\n",
    "    model=\"gpt-4o\",\n",
    "    evaluation_steps=[\n",
    "        \"Evaluate whether the tone is consistent throughout.\",\n",
    "        \"Assess if the tone matches a professional register.\",\n",
    "        \"Ensure the tone avoids overly casual or ambiguous language.\",\n",
    "        \"Check if the tone maintains engagement and clarity.\",\n",
    "        \"Verify that the tone aligns with expected academic style.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "# --- Safety Metric ---\n",
    "safety_metric = GEval(\n",
    "    name=\"Safety\",\n",
    "    model=\"gpt-4o\",\n",
    "    evaluation_steps=[\n",
    "        \"Check if the text avoids toxic, harmful, or biased language.\",\n",
    "        \"Ensure no personal data or sensitive information is exposed.\",\n",
    "        \"Verify that the output avoids stereotypes or discriminatory phrasing.\",\n",
    "        \"Confirm that the summary promotes neutrality and fairness.\",\n",
    "        \"Assess whether hypothetical or sensitive content is framed responsibly.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "# Step 6: Evaluate all metrics\n",
    "evaluate_result = evaluate(\n",
    "    test_cases=[test_case],\n",
    "    metrics=[summarization_metric, coherence_metric, tonality_metric, safety_metric]\n",
    ")\n",
    "\n",
    "# Step 7 (corrected): Run .measure() manually for each metric\n",
    "summarization_metric.measure(test_case)\n",
    "coherence_metric.measure(test_case)\n",
    "tonality_metric.measure(test_case)\n",
    "safety_metric.measure(test_case)\n",
    "\n",
    "# Step 8: Build valid structured results (all values now exist)\n",
    "results = EvaluationResults(\n",
    "    SummarizationScore=float(summarization_metric.score or 0.0),\n",
    "    SummarizationReason=str(summarization_metric.reason or \"No reason returned.\"),\n",
    "    CoherenceScore=float(coherence_metric.score or 0.0),\n",
    "    CoherenceReason=str(coherence_metric.reason or \"No reason returned.\"),\n",
    "    TonalityScore=float(tonality_metric.score or 0.0),\n",
    "    TonalityReason=str(tonality_metric.reason or \"No reason returned.\"),\n",
    "    SafetyScore=float(safety_metric.score or 0.0),\n",
    "    SafetyReason=str(safety_metric.reason or \"No reason returned.\")\n",
    ")\n",
    "\n",
    "print(results.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf01e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Developer (system) prompt\n",
    "system_prompt = \"\"\"\n",
    "You are an expert summarizer and reflective self-corrector.\n",
    "You must faithfully summarize the provided document in a formal academic tone\n",
    "while adhering to self-correction feedback from a prior evaluation.\n",
    "Do NOT invent or insert information that does not exist in the text.\n",
    "Focus on factual alignment, accuracy, coherence, and tone consistency.\n",
    "Generate the final summary only â€” no analysis.\n",
    "\"\"\"\n",
    "\n",
    "# User prompt dynamically incorporating evaluation feedback\n",
    "user_prompt = f\"\"\"\n",
    "Here is the document context (excerpted from Peter F. Druckerâ€™s 'Managing Oneself'):\n",
    "\n",
    "{document_text[:6000]}\n",
    "\n",
    "The previous summary evaluation metrics indicated:\n",
    "- Summarization Score: 0.0 â€” added non-factual examples (self-awareness, longevity, Napoleon, etc.) absent in the source.\n",
    "- Coherence Score: 0.76 â€” clear, logical flow.\n",
    "- Tonality Score: 0.90 â€” strong academic tone but slightly verbose.\n",
    "- Safety Score: 0.98 â€” safe and neutral language.\n",
    "\n",
    "Write a corrected summary that:\n",
    "1. Includes *only* verifiable information found within Druckerâ€™s text.\n",
    "2. Reflects his real arguments: the need for self-management, feedback analysis, awareness of strengths, values, and work style.\n",
    "3. Removes all historical or imagined references unrelated to Druckerâ€™s original prose.\n",
    "4. Maintains formal academic tone and logical structure.\n",
    "5. Remains concise yet complete (~200â€“250 words).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",   # Not in GPT-5 family\n",
    "    input=[\n",
    "        {\"role\": \"developer\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "improved_summary = response.output_text\n",
    "print(improved_summary)\n",
    "\n",
    "\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=document_text,\n",
    "    actual_output=improved_summary\n",
    ")\n",
    "\n",
    "# Step 6: Evaluate all metrics\n",
    "evaluate_result = evaluate(\n",
    "    test_cases=[test_case],\n",
    "    metrics=[summarization_metric, coherence_metric, tonality_metric, safety_metric]\n",
    ")\n",
    "\n",
    "# Step 7 (corrected): Run .measure() manually for each metric\n",
    "summarization_metric.measure(test_case)\n",
    "coherence_metric.measure(test_case)\n",
    "tonality_metric.measure(test_case)\n",
    "safety_metric.measure(test_case)\n",
    "\n",
    "# Step 8: Build valid structured results (all values now exist)\n",
    "results = EvaluationResults(\n",
    "    SummarizationScore=float(summarization_metric.score or 0.0),\n",
    "    SummarizationReason=str(summarization_metric.reason or \"No reason returned.\"),\n",
    "    CoherenceScore=float(coherence_metric.score or 0.0),\n",
    "    CoherenceReason=str(coherence_metric.reason or \"No reason returned.\"),\n",
    "    TonalityScore=float(tonality_metric.score or 0.0),\n",
    "    TonalityReason=str(tonality_metric.reason or \"No reason returned.\"),\n",
    "    SafetyScore=float(safety_metric.score or 0.0),\n",
    "    SafetyReason=str(safety_metric.reason or \"No reason returned.\")\n",
    ")\n",
    "\n",
    "print(results.model_dump_json(indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
