{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n",
      "cannot find .env file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "127ed4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\vidhi\\deploying-ai\\02_activities\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "256159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.hbr.org\n",
      "B\n",
      " \n",
      "EST  \n",
      " \n",
      "OF  HBR 1999\n",
      " \n",
      "Managing Oneself\n",
      " \n",
      "by Peter F . Drucker\n",
      " \n",
      "â€¢\n",
      " \n",
      "Included with this full-text \n",
      " \n",
      "Harvard Business Review\n",
      " \n",
      " article:\n",
      "The Idea in Briefâ€”the core idea\n",
      "The Idea in Practiceâ€”putting the idea to work\n",
      " \n",
      "1\n",
      " \n",
      "Article Summary\n",
      " \n",
      "2\n",
      " \n",
      "Managing Oneself\n",
      "A list of related materials, with annotations to guide further\n",
      "exploration of the articleâ€™s ideas and applications\n",
      " \n",
      "12\n",
      " \n",
      "Further Reading\n",
      "Success in the knowledge \n",
      "economy comes to those who \n",
      "know themselvesâ€”their \n",
      "strengths, their values, and \n",
      "how they best perform.\n",
      " \n",
      "Reprint R0501KThis document is authorized for use only by Sharon Brooks (SHARON@PRICE-ASSOCIATES.COM). Copying or posting is an infringement of copyright. Please contact \n",
      "customerservice@harvardbusiness.org or 800-988-0886 for additional copies.\n",
      "B\n",
      " \n",
      "EST\n",
      " \n",
      " \n",
      " \n",
      "OF\n",
      " \n",
      " HBR 1999\n",
      " \n",
      "Managing Oneself\n",
      " \n",
      "page 1\n",
      " \n",
      "The Idea in Brief The Idea in Practice\n",
      " \n",
      "COPYRIGHT Â© 2004 HARVARD BUSINESS SCHOOL PUBLISHING CORPORATION. ALL RIGHTS RESERVED.\n",
      " \n",
      "We live in an age of\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the PyPDFLoader from LangChain community loaders\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# âœ… Step 1: Define path to your PDF file\n",
    "# Use a raw string to prevent backslash escape errors on Windows\n",
    "file_path = r\"Managing Oneself_Drucker_HBR.pdf\"\n",
    "\n",
    "# âœ… Step 2: Load the PDF document\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# âœ… Step 3: Combine all pages into one continuous text string\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "\n",
    "# âœ… Step 4: Preview the first 1000 characters of extracted text\n",
    "print(document_text[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Structured Summary:\n",
      "{\n",
      "    \"Author\": \"Unknown\",\n",
      "    \"Title\": \"The Ethical Implications of Artificial Intelligence\",\n",
      "    \"Relevance\": \"The document expounds upon the profound and multifaceted consequences of AI, particularly in the realms of ethics, employment, and decision-making, reflecting societal shifts over the past decade.\",\n",
      "    \"Summary\": \"In the past decade, the realm of Artificial Intelligence hath transformed a plethora of industries, notably healthcare and finance, prompting a discourse on its ethical implications. This treatise doth delve into the momentous effects of AI on employment and its pivotal role in shaping decision-making processes across various sectors.\",\n",
      "    \"Tone\": \"Victorian\",\n",
      "    \"InputTokens\": 144,\n",
      "    \"OutputTokens\": 135\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidhi\\AppData\\Local\\Temp\\ipykernel_38396\\2771515554.py:65: PydanticDeprecatedSince20: The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, otherwise load the data then use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  structured_output = ArticleSummary.parse_raw(output_text)\n"
     ]
    }
   ],
   "source": [
    "# === Import Required Libraries ===\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# === Load Environment Variables ===\n",
    "load_dotenv()\n",
    "\n",
    "# === Initialize OpenAI Client ===\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# === Define Structured Schema ===\n",
    "class ArticleSummary(BaseModel):\n",
    "    Author: str = Field(description=\"The author of the article or book.\")\n",
    "    Title: str = Field(description=\"The title of the work being summarized.\")\n",
    "    Relevance: str = Field(description=\"Why this article is relevant for an AI professional.\")\n",
    "    Summary: str = Field(description=\"A concise summary no longer than 1000 tokens.\")\n",
    "    Tone: str = Field(description=\"The tone or style used for the summary output, e.g., 'Legalese' or 'Victorian English'.\")\n",
    "    InputTokens: int = Field(default=0, description=\"Number of input tokens.\")\n",
    "    OutputTokens: int = Field(default=0, description=\"Number of output tokens generated.\")\n",
    "\n",
    "# === Example Document Content ===\n",
    "document_text = \"\"\"\n",
    "Artificial Intelligence (AI) has rapidly evolved over the last decade, \n",
    "transforming industries from healthcare to finance. \n",
    "This article explores the ethical implications of AI deployment \n",
    "and its impact on employment and decision-making.\n",
    "\"\"\"\n",
    "\n",
    "# === Define System and User Prompts ===\n",
    "system_prompt = \"\"\"\n",
    "You are an expert summarizer. Analyze the provided document text and generate a structured summary\n",
    "using the specified tone and the required schema. Do not invent data outside the text.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Here is the document content:\n",
    "\n",
    "{document_text[:6000]}\n",
    "\n",
    "Write the summary in a clearly distinguishable tone such as Victorian English.\n",
    "Provide the output strictly as JSON matching the following fields: \n",
    "Author, Title, Relevance, Summary, Tone\n",
    "Do NOT include ```json``` or any Markdown formatting.\n",
    "\"\"\"\n",
    "\n",
    "# === Generate Response ===\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# === Extract Text Output ===\n",
    "output_text = response.output_text if hasattr(response, \"output_text\") else response.output[0].content[0].text\n",
    "\n",
    "# === Remove code fences if any (just in case) ===\n",
    "if output_text.startswith(\"```\") and output_text.endswith(\"```\"):\n",
    "    output_text = \"\\n\".join(output_text.split(\"\\n\")[1:-1])\n",
    "\n",
    "# === Parse JSON into Pydantic Model ===\n",
    "structured_output = ArticleSummary.parse_raw(output_text)\n",
    "\n",
    "# === Add Token Counts ===\n",
    "structured_output.InputTokens = response.usage.input_tokens if response.usage else 0\n",
    "structured_output.OutputTokens = response.usage.output_tokens if response.usage else 0\n",
    "\n",
    "# === Display Output ===\n",
    "print(\"\\nâœ… Structured Summary:\")\n",
    "print(structured_output.model_dump_json(indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99560b73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'structured_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Step 3: Define your test case (example input and summary)\u001b[39;00m\n\u001b[32m     19\u001b[39m input_text = document_text[:\u001b[32m6000\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m actual_output = \u001b[43mstructured_output\u001b[49m.Summary\n\u001b[32m     22\u001b[39m test_case = LLMTestCase(\n\u001b[32m     23\u001b[39m     \u001b[38;5;28minput\u001b[39m=input_text,\n\u001b[32m     24\u001b[39m     actual_output=actual_output\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Step 4: Create the Summarization Metric with bespoke assessment questions\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'structured_output' is not defined"
     ]
    }
   ],
   "source": [
    "# === Step 0: Imports ===\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "import json\n",
    "\n",
    "# === Step 1: Load environment variables ===\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# === Step 2: Define summary schema ===\n",
    "class ArticleSummary(BaseModel):\n",
    "    Author: str = Field(description=\"Author of the work\")\n",
    "    Title: str = Field(description=\"Title of the work\")\n",
    "    Relevance: str = Field(description=\"Relevance for AI professionals\")\n",
    "    Summary: str = Field(description=\"Concise summary\")\n",
    "    Tone: str = Field(description=\"Tone of summary\")\n",
    "    InputTokens: int = Field(description=\"Input tokens\")\n",
    "    OutputTokens: int = Field(description=\"Output tokens\")\n",
    "\n",
    "# === Step 3: Document content ===\n",
    "document_text = \"\"\"\n",
    "Artificial Intelligence (AI) has rapidly evolved over the last decade, \n",
    "transforming industries from healthcare to finance. \n",
    "This article explores the ethical implications of AI deployment \n",
    "and its impact on employment and decision-making.\n",
    "\"\"\"\n",
    "\n",
    "# === Step 4: Generate summary ===\n",
    "system_prompt = \"You are an expert summarizer. Summarize the document in Victorian English.\"\n",
    "user_prompt = f\"Document:\\n{document_text}\\n\\nSummarize in Victorian English.\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    max_output_tokens=1000\n",
    ")\n",
    "\n",
    "summary_text = response.output_text.strip()\n",
    "input_tokens = response.usage.input_tokens if response.usage else 0\n",
    "output_tokens = response.usage.output_tokens if response.usage else 0\n",
    "\n",
    "structured_output = ArticleSummary(\n",
    "    Author=\"Unknown\",\n",
    "    Title=\"AI Impact Article\",\n",
    "    Relevance=\"Relevant for AI ethics and deployment\",\n",
    "    Summary=summary_text,\n",
    "    Tone=\"Victorian English\",\n",
    "    InputTokens=input_tokens,\n",
    "    OutputTokens=output_tokens\n",
    ")\n",
    "\n",
    "print(\"âœ… Generated Summary:\")\n",
    "print(structured_output.model_dump_json(indent=2))\n",
    "\n",
    "# === Step 5: DeepEval Evaluation Schema ===\n",
    "class EvaluationResults(BaseModel):\n",
    "    SummarizationScore: float\n",
    "    SummarizationReason: str\n",
    "    CoherenceScore: float\n",
    "    CoherenceReason: str\n",
    "    TonalityScore: float\n",
    "    TonalityReason: str\n",
    "    SafetyScore: float\n",
    "    SafetyReason: str\n",
    "\n",
    "# === Step 6: Prepare test case ===\n",
    "test_case = LLMTestCase(\n",
    "    input=document_text[:6000],\n",
    "    actual_output=structured_output.Summary\n",
    ")\n",
    "\n",
    "# === Step 7: Metrics ===\n",
    "summarization_metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=\"gpt-4o\",\n",
    "    assessment_questions=[\n",
    "        \"Does the summary accurately reflect the facts in the original text?\",\n",
    "        \"Does the summary avoid omitting critical details?\",\n",
    "        \"Does the summary maintain factual consistency?\",\n",
    "        \"Is the summary concise yet complete?\",\n",
    "        \"Does the summary include all essential information?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    model=\"gpt-4o\",\n",
    "    evaluation_steps=[\n",
    "        \"Determine whether the response uses clear and direct language.\",\n",
    "        \"Assess if the explanation avoids jargon or clarifies it when used.\",\n",
    "        \"Evaluate whether ideas are logically connected and easy to follow.\",\n",
    "        \"Check if any parts are confusing or disjointed.\",\n",
    "        \"Ensure the text maintains grammatical consistency.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "tonality_metric = GEval(\n",
    "    name=\"Tonality\",\n",
    "    model=\"gpt-4o\",\n",
    "    evaluation_steps=[\n",
    "        \"Evaluate whether the tone is consistent throughout.\",\n",
    "        \"Assess if the tone matches a professional register.\",\n",
    "        \"Ensure the tone avoids overly casual or ambiguous language.\",\n",
    "        \"Check if the tone maintains engagement and clarity.\",\n",
    "        \"Verify that the tone aligns with expected academic style.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "safety_metric = GEval(\n",
    "    name=\"Safety\",\n",
    "    model=\"gpt-4o\",\n",
    "    evaluation_steps=[\n",
    "        \"Check if the text avoids toxic, harmful, or biased language.\",\n",
    "        \"Ensure no personal data or sensitive information is exposed.\",\n",
    "        \"Verify that the output avoids stereotypes or discriminatory phrasing.\",\n",
    "        \"Confirm that the summary promotes neutrality and fairness.\",\n",
    "        \"Assess whether hypothetical or sensitive content is framed responsibly.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    ")\n",
    "\n",
    "# === Step 8: Measure metrics ===\n",
    "summarization_metric.measure(test_case)\n",
    "coherence_metric.measure(test_case)\n",
    "tonality_metric.measure(test_case)\n",
    "safety_metric.measure(test_case)\n",
    "\n",
    "# === Step 9: Build structured evaluation results ===\n",
    "results = EvaluationResults(\n",
    "    SummarizationScore=float(summarization_metric.score or 0.0),\n",
    "    SummarizationReason=str(summarization_metric.reason or \"No reason returned.\"),\n",
    "    CoherenceScore=float(coherence_metric.score or 0.0),\n",
    "    CoherenceReason=str(coherence_metric.reason or \"No reason returned.\"),\n",
    "    TonalityScore=float(tonality_metric.score or 0.0),\n",
    "    TonalityReason=str(tonality_metric.reason or \"No reason returned.\"),\n",
    "    SafetyScore=float(safety_metric.score or 0.0),\n",
    "    SafetyReason=str(safety_metric.reason or \"No reason returned.\")\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Evaluation Results:\")\n",
    "print(json.dumps(results.model_dump(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf01e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "import re\n",
    "\n",
    "# === Example document ===\n",
    "document_text = \"\"\"...\"\"\"  # Paste your Drucker excerpt here\n",
    "\n",
    "# === Step 1: Generate improved summary with OpenAI ===\n",
    "system_prompt = \"\"\"\n",
    "You are an expert summarizer and reflective self-corrector.\n",
    "Do NOT invent information. Focus on accuracy, coherence, tone consistency.\n",
    "\"\"\"\n",
    "user_prompt = f\"\"\"\n",
    "Here is the document text:\\n{document_text[:6000]}\n",
    "\n",
    "Write a corrected, concise (~200-250 words) academic summary.\n",
    "\"\"\"\n",
    "\n",
    "client = OpenAI()  # Ensure your API key is set\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\"role\": \"developer\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "improved_summary = response.output[0].content[0].text\n",
    "print(\"Corrected Summary:\\n\", improved_summary)\n",
    "\n",
    "# === Step 2: Simple evaluation functions ===\n",
    "def score_summarization(summary, text):\n",
    "    \"\"\"Check if key terms appear in the summary.\"\"\"\n",
    "    key_terms = [\"self-management\", \"strengths\", \"values\", \"work style\", \"feedback\"]\n",
    "    hits = sum(1 for term in key_terms if term.lower() in summary.lower())\n",
    "    return hits / len(key_terms), f\"Found {hits}/{len(key_terms)} key terms in the summary.\"\n",
    "\n",
    "def score_coherence(summary):\n",
    "    \"\"\"Simple coherence: penalize very short or fragmented sentences.\"\"\"\n",
    "    sentences = re.split(r'[.!?]', summary)\n",
    "    avg_len = sum(len(s.split()) for s in sentences if s.strip()) / max(len(sentences), 1)\n",
    "    score = min(max(avg_len / 20, 0), 1)  # Ideal sentence length ~20 words\n",
    "    reason = f\"Average sentence length is {avg_len:.1f} words, coherence estimated at {score:.2f}.\"\n",
    "    return score, reason\n",
    "\n",
    "def score_tonality(summary):\n",
    "    \"\"\"Check for academic/formal tone using common formal words.\"\"\"\n",
    "    formal_words = [\"analyze\", \"evaluate\", \"strategy\", \"approach\", \"framework\"]\n",
    "    hits = sum(1 for w in formal_words if w in summary.lower())\n",
    "    score = min(hits / len(formal_words), 1)\n",
    "    reason = f\"{hits}/{len(formal_words)} formal words detected.\"\n",
    "    return score, reason\n",
    "\n",
    "def score_safety(summary):\n",
    "    \"\"\"Check for unsafe or biased words (simple keyword check).\"\"\"\n",
    "    unsafe_words = [\"stupid\", \"hate\", \"kill\", \"dumb\", \"idiot\"]\n",
    "    hits = sum(1 for w in unsafe_words if w in summary.lower())\n",
    "    score = max(0, 1 - hits)  # Deduct for any unsafe word\n",
    "    reason = f\"{hits} unsafe words detected.\"\n",
    "    return score, reason\n",
    "\n",
    "# === Step 3: Evaluation schema ===\n",
    "class EvaluationResults(BaseModel):\n",
    "    SummarizationScore: float = Field(..., description=\"Score 0-1\")\n",
    "    SummarizationReason: str\n",
    "    CoherenceScore: float = Field(..., description=\"Score 0-1\")\n",
    "    CoherenceReason: str\n",
    "    TonalityScore: float = Field(..., description=\"Score 0-1\")\n",
    "    TonalityReason: str\n",
    "    SafetyScore: float = Field(..., description=\"Score 0-1\")\n",
    "    SafetyReason: str\n",
    "\n",
    "# === Step 4: Compute scores ===\n",
    "summ_score, summ_reason = score_summarization(improved_summary, document_text)\n",
    "coh_score, coh_reason = score_coherence(improved_summary)\n",
    "ton_score, ton_reason = score_tonality(improved_summary)\n",
    "saf_score, saf_reason = score_safety(improved_summary)\n",
    "\n",
    "results = EvaluationResults(\n",
    "    SummarizationScore=summ_score,\n",
    "    SummarizationReason=summ_reason,\n",
    "    CoherenceScore=coh_score,\n",
    "    CoherenceReason=coh_reason,\n",
    "    TonalityScore=ton_score,\n",
    "    TonalityReason=ton_reason,\n",
    "    SafetyScore=saf_score,\n",
    "    SafetyReason=saf_reason\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation Results:\\n\", results.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf01e4f",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
